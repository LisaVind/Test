{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/LisaVind/Test/blob/main/pyTorchGuide.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjqsTA2iy3Pi"
      },
      "source": [
        "## Pytorch Guide"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "o5VOQNT1y3Pj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import torchvision.transforms as T\n",
        "from torchvision.io import read_image\n",
        "from torch import nn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ggDjlWwhy3Pk"
      },
      "source": [
        "### Tensor initialization\n",
        "Tensors can be created directly from data. The data type is automatically inferred."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NX1Ge2kuy3Pk"
      },
      "outputs": [],
      "source": [
        "data = [[1, 2], [3, 4]]\n",
        "x_data = torch.tensor(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lml8T2siy3Pk"
      },
      "source": [
        "From a NumPy array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "0BtQd6twy3Pk"
      },
      "outputs": [],
      "source": [
        "np_array = np.array(data)\n",
        "x_np = torch.from_numpy(np_array)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOrAL0G8y3Pl"
      },
      "source": [
        "From another tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvsdxCwVy3Pl",
        "outputId": "162084c5-c6c7-4eb0-a8a9-817f094e4715"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ones Tensor: \n",
            " tensor([[1, 1],\n",
            "        [1, 1]]) \n",
            "\n",
            "Random Tensor: \n",
            " tensor([[0.1662, 0.1721],\n",
            "        [0.6992, 0.5991]]) \n",
            "\n"
          ]
        }
      ],
      "source": [
        "x_ones = torch.ones_like(x_data) # retains the properties of x_data\n",
        "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
        "\n",
        "x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data\n",
        "print(f\"Random Tensor: \\n {x_rand} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PNzxQvIy3Pl"
      },
      "source": [
        "With random or constant values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQHVej_Gy3Pl",
        "outputId": "6288907f-ab02-43c5-edb3-a59284101fac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random Tensor: \n",
            " tensor([[0.0922, 0.2886, 0.3885],\n",
            "        [0.7203, 0.3714, 0.1395]]) \n",
            "\n",
            "Ones Tensor: \n",
            " tensor([[1., 1., 1.],\n",
            "        [1., 1., 1.]]) \n",
            "\n",
            "Zeros Tensor: \n",
            " tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "shape = (2, 3,)\n",
        "rand_tensor = torch.rand(shape)\n",
        "ones_tensor = torch.ones(shape)\n",
        "zeros_tensor = torch.zeros(shape)\n",
        "\n",
        "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
        "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
        "print(f\"Zeros Tensor: \\n {zeros_tensor}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9qXL48ITy3Pl"
      },
      "source": [
        "### Tensor attributes\n",
        "Tensor attributes describe their shape, datatype, and the device on which they are stored."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Lg4gGX7y3Pl",
        "outputId": "b1ed1a42-41e1-4751-cbf9-886bbe51cfb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of tensor: torch.Size([3, 4])\n",
            "Datatype of tensor: torch.float32\n",
            "Device tensor is stored on: cpu\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.rand(3, 4)\n",
        "\n",
        "print(f\"Shape of tensor: {tensor.shape}\")\n",
        "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
        "print(f\"Device tensor is stored on: {tensor.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kH9foqiCy3Pm"
      },
      "source": [
        "### Tensor Operations\n",
        "Over 100 tensor operations, including transposing, indexing, slicing, mathematical operations, linear algebra, random sampling are availible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "N_apQACyy3Pm"
      },
      "outputs": [],
      "source": [
        "# We move our tensor to the GPU if available\n",
        "if torch.cuda.is_available():\n",
        "  tensor = tensor.to('cuda')\n",
        "  print(f\"Device tensor is stored on: {tensor.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SE2IH5OKy3Pm"
      },
      "source": [
        "Indexing and slicing:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rVGvTWGy3Pm",
        "outputId": "0ec3be4d-6ef3-4484-f05d-78f69566757e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "tensor = torch.ones(4, 4)\n",
        "tensor[:,1] = 0\n",
        "print(tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npyhY481y3Pm"
      },
      "source": [
        "Joining tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mKMg67uyy3Pm",
        "outputId": "84f9f0c9-8170-4120-81d9-6fdc22f3edc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1., 1., 0., 1., 1., 1., 0., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "t1 = torch.cat([tensor, tensor, tensor], dim=1)\n",
        "print(t1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8OG3-ICy3Pm"
      },
      "source": [
        "Multiplying tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdODGXTTy3Pm",
        "outputId": "d72c9643-d5ab-4797-a72f-5a04b935cb80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor.mul(tensor) \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]]) \n",
            "\n",
            "tensor * tensor \n",
            " tensor([[1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.],\n",
            "        [1., 0., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "# This computes the element-wise product\n",
        "print(f\"tensor.mul(tensor) \\n {tensor.mul(tensor)} \\n\")\n",
        "# Alternative syntax:\n",
        "print(f\"tensor * tensor \\n {tensor * tensor}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMRqvG80y3Pm"
      },
      "source": [
        "### Loading in data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "glnYjCbdy3Pm",
        "outputId": "683515da-8188-4142-b8e4-a824fa6a76a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 22.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 628kB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 5.51MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 2.25MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "training_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.MNIST(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,\n",
        "    transform=ToTensor()\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JGMxatCRy3Pm"
      },
      "source": [
        "### Creating a Custom Dataset for your files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7MZzLJ4Zy3Pm"
      },
      "outputs": [],
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):\n",
        "        self.img_labels = pd.read_csv(annotations_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
        "        image = read_image(img_path)\n",
        "        label = self.img_labels.iloc[idx, 1]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        if self.target_transform:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk6E0SUQy3Pm"
      },
      "source": [
        "### Preparing your data for training with DataLoaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XyqIhZtgy3Pm"
      },
      "outputs": [],
      "source": [
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGLWQ7Aty3Pm"
      },
      "source": [
        "### Transformations\n",
        "For augmentation of data. You do not need to augment the validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Jb5oO8g-y3Pm"
      },
      "outputs": [],
      "source": [
        "transforms_train = T.Compose([\n",
        "  T.RandomHorizontalFlip(p=0.5),\n",
        "  T.GaussianBlur(kernel_size=(7, 13), sigma=(0.1, 0.2)),\n",
        "  T.Resize((224, 224)),\n",
        "  T.ToTensor(),\n",
        "  T.Normalize(mean = torch.tensor([0.504, 0.504, 0.503]) , std=torch.tensor([0.019 , 0.018, 0.018]))\n",
        "])\n",
        "\n",
        "# we do not augment the validation dataset (aside from resizing and tensor casting)\n",
        "transforms_val = T.Compose([\n",
        "  T.Resize((224, 224)),\n",
        "  T.ToTensor(),\n",
        "  T.Normalize(mean =torch.tensor([0.504, 0.504, 0.503]), std=torch.tensor([0.019 , 0.018, 0.018]))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fi50ESguy3Pm"
      },
      "source": [
        "### Defining neural network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "G4pTFydTy3Pm"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(28*28, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 10),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yVQwq_gy3Pm"
      },
      "source": [
        "### Define device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxvms3GDy3Pm",
        "outputId": "e34e7ab0-508a-41c6-a385-5a3a5be746ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n"
          ]
        }
      ],
      "source": [
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1ynmOmyy3Pm"
      },
      "source": [
        "### Create instance of model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "pmJewPeLy3Pm"
      },
      "outputs": [],
      "source": [
        "model = NeuralNetwork().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDRIMBxOy3Pm"
      },
      "source": [
        "### Preforming a forward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Z-pFV8pry3Pm"
      },
      "outputs": [],
      "source": [
        "idx = np.random.randint(len(training_data)) # random sample\n",
        "x, y = training_data[idx]\n",
        "\n",
        "# Perform a forward pass\n",
        "logits = model.forward(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KY9nkcumy3Pm"
      },
      "source": [
        "### Define loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "Ylzlhee6y3Pn"
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDja_TiYy3Pn"
      },
      "source": [
        "### Training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "81POlKyZy3Pn"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "mP2WRCYyy3Pn"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    # Set the model to evaluation mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    test_loss, correct = 0, 0\n",
        "\n",
        "    # Evaluating the model with torch.no_grad() ensures that no gradients are computed during test mode\n",
        "    # also serves to reduce unnecessary gradient computations and memory usage for tensors with requires_grad=True\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRpXvyLoy3Pn",
        "outputId": "06f048c2-8f18-4584-d9cc-aa1ec0a55ea5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.309171  [   64/60000]\n",
            "loss: 2.298182  [ 6464/60000]\n",
            "loss: 2.284835  [12864/60000]\n",
            "loss: 2.291518  [19264/60000]\n",
            "loss: 2.288711  [25664/60000]\n",
            "loss: 2.269095  [32064/60000]\n",
            "loss: 2.278391  [38464/60000]\n",
            "loss: 2.266201  [44864/60000]\n",
            "loss: 2.260248  [51264/60000]\n",
            "loss: 2.263827  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 34.7%, Avg loss: 2.255456 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.253743  [   64/60000]\n",
            "loss: 2.243757  [ 6464/60000]\n",
            "loss: 2.240140  [12864/60000]\n",
            "loss: 2.256401  [19264/60000]\n",
            "loss: 2.238879  [25664/60000]\n",
            "loss: 2.222037  [32064/60000]\n",
            "loss: 2.221849  [38464/60000]\n",
            "loss: 2.208062  [44864/60000]\n",
            "loss: 2.197653  [51264/60000]\n",
            "loss: 2.198611  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 50.4%, Avg loss: 2.187327 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.202874  [   64/60000]\n",
            "loss: 2.193469  [ 6464/60000]\n",
            "loss: 2.173721  [12864/60000]\n",
            "loss: 2.155449  [19264/60000]\n",
            "loss: 2.176821  [25664/60000]\n",
            "loss: 2.153502  [32064/60000]\n",
            "loss: 2.107680  [38464/60000]\n",
            "loss: 2.118779  [44864/60000]\n",
            "loss: 2.059826  [51264/60000]\n",
            "loss: 2.084081  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.2%, Avg loss: 2.071648 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.083862  [   64/60000]\n",
            "loss: 2.045640  [ 6464/60000]\n",
            "loss: 2.031589  [12864/60000]\n",
            "loss: 2.059584  [19264/60000]\n",
            "loss: 1.996580  [25664/60000]\n",
            "loss: 2.025264  [32064/60000]\n",
            "loss: 1.999452  [38464/60000]\n",
            "loss: 1.986928  [44864/60000]\n",
            "loss: 1.940627  [51264/60000]\n",
            "loss: 1.960389  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.9%, Avg loss: 1.875491 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.949902  [   64/60000]\n",
            "loss: 1.800901  [ 6464/60000]\n",
            "loss: 1.860400  [12864/60000]\n",
            "loss: 1.789991  [19264/60000]\n",
            "loss: 1.785831  [25664/60000]\n",
            "loss: 1.808290  [32064/60000]\n",
            "loss: 1.775812  [38464/60000]\n",
            "loss: 1.665449  [44864/60000]\n",
            "loss: 1.613843  [51264/60000]\n",
            "loss: 1.612278  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.4%, Avg loss: 1.582158 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 1.582472  [   64/60000]\n",
            "loss: 1.622676  [ 6464/60000]\n",
            "loss: 1.508402  [12864/60000]\n",
            "loss: 1.568271  [19264/60000]\n",
            "loss: 1.368621  [25664/60000]\n",
            "loss: 1.399979  [32064/60000]\n",
            "loss: 1.344749  [38464/60000]\n",
            "loss: 1.307038  [44864/60000]\n",
            "loss: 1.409366  [51264/60000]\n",
            "loss: 1.328631  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 76.5%, Avg loss: 1.260302 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 1.375090  [   64/60000]\n",
            "loss: 1.329539  [ 6464/60000]\n",
            "loss: 1.233069  [12864/60000]\n",
            "loss: 1.121958  [19264/60000]\n",
            "loss: 1.167167  [25664/60000]\n",
            "loss: 1.271663  [32064/60000]\n",
            "loss: 1.066121  [38464/60000]\n",
            "loss: 1.132136  [44864/60000]\n",
            "loss: 0.982258  [51264/60000]\n",
            "loss: 1.045360  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 79.7%, Avg loss: 1.006664 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.091399  [   64/60000]\n",
            "loss: 1.077761  [ 6464/60000]\n",
            "loss: 0.930496  [12864/60000]\n",
            "loss: 0.909404  [19264/60000]\n",
            "loss: 1.124394  [25664/60000]\n",
            "loss: 0.950109  [32064/60000]\n",
            "loss: 0.976329  [38464/60000]\n",
            "loss: 0.797064  [44864/60000]\n",
            "loss: 0.842413  [51264/60000]\n",
            "loss: 0.893913  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 81.7%, Avg loss: 0.836701 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 0.846107  [   64/60000]\n",
            "loss: 0.789959  [ 6464/60000]\n",
            "loss: 0.925990  [12864/60000]\n",
            "loss: 0.784869  [19264/60000]\n",
            "loss: 0.780579  [25664/60000]\n",
            "loss: 0.668424  [32064/60000]\n",
            "loss: 0.633550  [38464/60000]\n",
            "loss: 0.776389  [44864/60000]\n",
            "loss: 0.704223  [51264/60000]\n",
            "loss: 0.802837  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 83.2%, Avg loss: 0.722791 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.740175  [   64/60000]\n",
            "loss: 0.821113  [ 6464/60000]\n",
            "loss: 0.781664  [12864/60000]\n",
            "loss: 0.789065  [19264/60000]\n",
            "loss: 0.528132  [25664/60000]\n",
            "loss: 0.865960  [32064/60000]\n",
            "loss: 0.712760  [38464/60000]\n",
            "loss: 0.720473  [44864/60000]\n",
            "loss: 0.591630  [51264/60000]\n",
            "loss: 0.535780  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 84.3%, Avg loss: 0.643252 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g32Ty9Jwy3Pn"
      },
      "source": [
        "### Training epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "5yC3WnXQy3Pn"
      },
      "outputs": [],
      "source": [
        "def train_epoch(data_loader, model, optimiser, device):\n",
        "\n",
        "  # set model to training mode. This is important because some layers behave differently during training and testing\n",
        "  model.train(True)\n",
        "  model.to(device)\n",
        "\n",
        "  # stats\n",
        "  loss_total = 0.0\n",
        "  oa_total = 0.0\n",
        "\n",
        "  for idx, (data, target) in enumerate(data_loader):\n",
        "\n",
        "    #TODO: implement the training step here. Check the introductory slides if you need help.\n",
        "\n",
        "    # put data and target onto correct device\n",
        "    data, target = data.to(device), target.to(device)\n",
        "\n",
        "    # reset gradients\n",
        "    optimiser.zero_grad()\n",
        "\n",
        "    # forward pass\n",
        "    pred = model(data)\n",
        "\n",
        "    # loss or criterion\n",
        "    loss = loss_fn(pred, target)\n",
        "\n",
        "    # backward pass (computing gradients)\n",
        "    loss.backward()\n",
        "\n",
        "    # parameter update\n",
        "    optimiser.step()\n",
        "\n",
        "    # stats update\n",
        "    loss_total += loss.item()\n",
        "    oa_total += torch.mean((pred.argmax(1) == target).float()).item()\n",
        "\n",
        "  # normalise stats\n",
        "  loss_total /= len(data_loader)\n",
        "  oa_total /= len(data_loader)\n",
        "\n",
        "  return model, loss_total, oa_total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qQeXpe8py3Pn"
      },
      "source": [
        "### Validation epoch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "xgYET78Zy3Pn"
      },
      "outputs": [],
      "source": [
        "def validate_epoch(data_loader, model, device):       # note: no optimiser needed\n",
        "\n",
        "  # set model to evaluation mode\n",
        "  model.train(False)\n",
        "  model.to(device)\n",
        "\n",
        "  # stats\n",
        "  loss_total = 0.0\n",
        "  oa_total = 0.0\n",
        "\n",
        "  for idx, (data, target) in enumerate(data_loader):\n",
        "    with torch.no_grad():\n",
        "      # put data and target onto correct device\n",
        "      data, target = data.to(device), target.to(device)\n",
        "\n",
        "      # forward pass\n",
        "      pred = model(data)\n",
        "\n",
        "      # loss\n",
        "      loss = criterion(pred, target)\n",
        "\n",
        "      # stats update\n",
        "      loss_total += loss.item()\n",
        "      oa_total += torch.mean((pred.argmax(1) == target).float()).item()\n",
        "\n",
        "  # normalise stats\n",
        "  loss_total /= len(data_loader)\n",
        "  oa_total /= len(data_loader)\n",
        "\n",
        "  return loss_total, oa_total"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xaa8nkoay3Pn"
      },
      "source": [
        "### Disabling gradient tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0H9bvtz6y3Pn"
      },
      "source": [
        "By default, all tensors with requires_grad=True are tracking their computational history and support gradient computation. However, there are some cases when we do not need to do that, for example, when we have trained the model and just want to apply it to some input data, i.e. we only want to do forward computations through the network. We can stop tracking computations by surrounding our computation code with torch.no_grad() block."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "zlcLJmUHy3Pn"
      },
      "outputs": [],
      "source": [
        "x = torch.ones(5)  # input tensor\n",
        "y = torch.zeros(3)  # expected output\n",
        "w = torch.randn(5, 3, requires_grad=True)\n",
        "b = torch.randn(3, requires_grad=True)\n",
        "z = torch.matmul(x, w)+b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1P-LHnfmy3Pn",
        "outputId": "8307f35c-0c5a-4779-e96a-85ae086835a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print(z.requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9y1nhb9y3Po",
        "outputId": "5e6b629b-4921-4eb8-8316-2b05cea56be8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "z = torch.matmul(x, w)+b\n",
        "print(z.requires_grad)\n",
        "\n",
        "#or use detach\n",
        "z = z.detach()\n",
        "print(z.requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QMgjDC3TzFOi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "envuni",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}